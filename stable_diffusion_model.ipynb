{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RV-E7RrrgPa5",
        "outputId": "0c743c16-7989-40a6-d703-162bbbfd1807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tqdm matplotlib requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "import os\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as dataloaders\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "BSoIlfsCgq0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rarfile\n",
        "\n",
        "# import os\n",
        "# import rarfile\n",
        "\n",
        "# # Create dataset directory first\n",
        "# dataset_folder = './dataset'\n",
        "# os.makedirs(dataset_folder, exist_ok=True)\n",
        "# print(f\"✓ Created dataset folder: {dataset_folder}\")\n",
        "\n",
        "# # Download the RAR file (using a clean output filename)\n",
        "# rar_filename = \"images.rar\"\n",
        "# download_url = \"https://chaladze.com/l5/img/Linnaeus%205%20128X128.rar\"\n",
        "\n",
        "# print(f\"Downloading {rar_filename}...\")\n",
        "# !wget -O {rar_filename} \"{download_url}\"\n",
        "\n",
        "# # Extract contents\n",
        "# rar_path = rar_filename\n",
        "# if os.path.exists(rar_path):\n",
        "#     try:\n",
        "#         rf = rarfile.RarFile(rar_path)\n",
        "#         rf.extractall(dataset_folder)\n",
        "#         print(f\"✓ Extraction complete! Files extracted to {dataset_folder}\")\n",
        "\n",
        "#         # Verify extraction and show first few files\n",
        "#         print(\"\\nExtracted files:\")\n",
        "#         extracted_files = []\n",
        "#         for root, dirs, files in os.walk(dataset_folder):\n",
        "#             for file in files[:10]:  # Show first 10 files as sample\n",
        "#                 print(f\"  {os.path.join(root, file)}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"✗ Extraction error: {e}\")\n",
        "# else:\n",
        "#     print(f\"✗ Error: RAR file not found at {rar_path}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NyVEZL9uNFXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGUalylMNpB_",
        "outputId": "5b930255-4fc7-486b-8b86-f86155d29c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_dump = '/content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training'\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/dataset/stable_diffusion_training/images/train/dog'"
      ],
      "metadata": {
        "id": "JLczsfiXL9DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rarfile\n",
        "\n",
        "# import rarfile\n",
        "\n",
        "# # Download\n",
        "# !wget \"https://chaladze.com/l5/img/Linnaeus%205%20128X128.rar\"\n",
        "\n",
        "# # Extract\n",
        "# rar_path = 'images.rar'\n",
        "# if os.path.exists(rar_path):\n",
        "#     rf = rarfile.RarFile(rar_path)\n",
        "#     rf.extractall('./dataset')\n",
        "#     print(\"✓ Extraction complete!\")"
      ],
      "metadata": {
        "id": "isgfAg4DmATo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class DogDataset(Dataset):\n",
        "    def __init__(self, path, transform=None, max_length=None):\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.images = sorted(os.listdir(path))\n",
        "        if max_length is not None:\n",
        "            self.images = self.images[:max_length]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.path, self.images[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n"
      ],
      "metadata": {
        "id": "hfpDl3lvLEBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(path, max_length=None):\n",
        "    print(\"Data loader Started\")\n",
        "    data_sets = DogDataset(path, transform=transforms.ToTensor(), max_length=max_length)\n",
        "    # if max_length is not None:\n",
        "    #     data_sets = torch.utils.data.Subset(data_sets, range(max_length))\n",
        "    data_loader = dataloaders.DataLoader(data_sets, batch_size=32, shuffle=True)\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "8h_Epe3_sgpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "import os\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as dataloaders\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# VAE\n",
        "# Variavtional Auto Endcoder\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3, image_size=32, blocks=2):\n",
        "        super().__init__()\n",
        "        self.encoder = VAEEncoder(\n",
        "            in_channels=in_channels,\n",
        "            latent_dim=(image_size / (2 ** blocks)),\n",
        "            block_count=blocks\n",
        "        )\n",
        "\n",
        "        print(f\"In vae channels out : {self.encoder.out_channels}\")\n",
        "\n",
        "        latent_channels = self.encoder.out_channels\n",
        "\n",
        "        self.mean = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "        self.log_variance = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1) # Renamed to log_variance\n",
        "\n",
        "        self.decoder = VAEDecoder(\n",
        "            in_channels=latent_channels, out_channels=3, blocks=blocks\n",
        "        )\n",
        "\n",
        "    def get_out_channels(self):\n",
        "        return self.encoder.out_channels\n",
        "\n",
        "    def reparameterized_trick(self, mean, log_variance):\n",
        "        std = torch.exp(0.5 * log_variance) # Standard deviation\n",
        "        epsilon = torch.randn_like(std)\n",
        "        z = mean + std * epsilon\n",
        "        return z\n",
        "\n",
        "    def encode(self, image_tensor):\n",
        "        latent = self.encoder(image_tensor)\n",
        "\n",
        "        mean = self.mean(latent)\n",
        "        log_variance = self.log_variance(latent) # Get log_variance\n",
        "\n",
        "        z = self.reparameterized_trick(mean=mean, log_variance=log_variance)\n",
        "\n",
        "        return latent, z, mean, log_variance # Return log_variance\n",
        "\n",
        "    def decode(self, latent):\n",
        "        x = self.decoder(latent)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAEEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim, block_count=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        curr_channels = in_channels\n",
        "        for i in range(block_count):\n",
        "            out_channel = 64 * (2 ** i)\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=out_channel,\n",
        "                )\n",
        "            )\n",
        "            curr_channels = out_channel\n",
        "\n",
        "        self.out_channels = out_channel\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, image_tensor):\n",
        "        latent = image_tensor\n",
        "        for block in self.blocks:\n",
        "            latent = block(latent)\n",
        "        return latent\n",
        "\n",
        "\n",
        "class VAEDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=128, out_channels=3, blocks=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = 3 if i == (blocks - 1) else in_channels // 2\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=next_channels\n",
        "                )\n",
        "            )\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        x = latent\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input, output, mean, log_variance = x # Expect log_variance directly\n",
        "        reconstruction_loss = torch.mean((input - output) ** 2)\n",
        "        kl_loss = -0.5 * torch.sum(1 + log_variance - mean ** 2 - torch.exp(log_variance))\n",
        "        beta = 0.001\n",
        "        total_loss = reconstruction_loss + beta * kl_loss\n",
        "        return total_loss, reconstruction_loss, kl_loss"
      ],
      "metadata": {
        "id": "_t2aF7Dq9nFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train VAE\n",
        "train_losses = []\n",
        "recon_losses = []\n",
        "kl_losses = []\n",
        "\n",
        "\n",
        "\n",
        "def train_vae(config=None):\n",
        "    print(\"Training VAE...\")\n",
        "    dataloader = data_loader(dataset_path, max_length=200)\n",
        "    vae = VAE(in_channels=3, image_size=128, blocks=2)\n",
        "    vae_loss = VAELoss()\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
        "    epochs = 50\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      epochs_total_loss = 0.0\n",
        "      epochs_recon_loss = 0.0\n",
        "      epochs_kl_loss = 0.0\n",
        "\n",
        "      for batch_idx, data in enumerate(dataloader):\n",
        "          # print(f\"Epoch: {epoch}, Batch: {batch_idx} shape :{data.shape}\")\n",
        "          latent, z, z_mean, z_log_variance = vae.encode(data)  # Get z_log_variance directly\n",
        "          output = vae.decode(latent=latent)\n",
        "          loss, recon_loss, kl_loss = vae_loss([data, output, z_mean, z_log_variance]) # Pass z_log_variance\n",
        "          # print(f\"log_variance: {z_log_variance.shape} mean {z_mean.shape}\")\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          epochs_total_loss += loss.item()\n",
        "          epochs_recon_loss += recon_loss.item()\n",
        "          epochs_kl_loss += kl_loss.item()\n",
        "\n",
        "      avg_loss = epochs_total_loss / len(dataloader)\n",
        "      avg_recon = epochs_recon_loss / len(dataloader)\n",
        "      avg_kl = epochs_kl_loss / len(dataloader)\n",
        "\n",
        "      train_losses.append(avg_loss)\n",
        "      recon_losses.append(avg_recon)\n",
        "      kl_losses.append(avg_kl)\n",
        "\n",
        "    print(\"VAE training completed.\")\n",
        "    dump_path = models_dump + \"/vae_model.pth\"\n",
        "    torch.save(vae.state_dict(), dump_path)\n",
        "    print(f\"VAE model saved to {dump_path}\")\n",
        "\n",
        "    # Save losses for later use\n",
        "    torch.save(\n",
        "        {\n",
        "            'train_losses': train_losses,\n",
        "            'recon_losses': recon_losses,\n",
        "            'kl_losses': kl_losses\n",
        "        }, 'loss_history.pt'\n",
        "    )\n",
        "\n",
        "# train_vae()\n",
        "print(f\"loss {train_losses} recon {recon_losses} kl loss {kl_losses}\")"
      ],
      "metadata": {
        "id": "GSi3EnMz-aEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a9815c-85fe-484d-c919-160876524748",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss [] recon [] kl loss []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT vae training loss\n",
        "\n",
        "def plot_loss_curves():\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Subplot 1: Total Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_losses, label='Total Loss')\n",
        "    plt.title('Total Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Subplot 2: Reconstruction Loss\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(recon_losses, label='Reconstruction Loss', color='orange')\n",
        "    plt.title('Reconstruction Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Subplot 3: KL Divergence\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(kl_losses, label='KL Divergence', color='green')\n",
        "    plt.title('KL Divergence')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('vae_loss_curves.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Call after training\n",
        "plot_loss_curves()"
      ],
      "metadata": {
        "id": "TMpnHlNV-cxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "3f555eb2-dd3d-4f71-a12b-326270f92e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train stable diffusion model\n",
        "\n",
        "# Unet\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, blocks=2):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.blocks = blocks\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "\n",
        "        curr_channels = in_channels\n",
        "        # Encoder\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels * 2\n",
        "            self.encoder.append(EncoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "        # Bottleneck means convoltional layer without `max pooling`\n",
        "        self.bottleneck = EncoderBlock(curr_channels, curr_channels)\n",
        "\n",
        "        # Decoder\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels // 2\n",
        "            self.decoder.append(DecoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def sinusoidal_embedding(self, time_step, embedding_dim):\n",
        "      device = time_step.device\n",
        "      dtype = time_step.dtype\n",
        "      half_dim = embedding_dim // 2\n",
        "      emb = torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / (half_dim - 1)\n",
        "      emb = torch.exp(torch.arange(half_dim, device=device, dtype=dtype) * -emb)\n",
        "      emb = time_step[:, None] * emb[None, :]\n",
        "      emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
        "      return emb\n",
        "\n",
        "\n",
        "    def forward(self, z_t, time_step):\n",
        "      x = z_t\n",
        "      skip_connections = []\n",
        "      for enc in self.encoder:\n",
        "        x, s_c = enc(x)\n",
        "        skip_connections.append(s_c)\n",
        "\n",
        "      pos_encoding = self.sinusoidal_embedding(time_step, x.size(1))\n",
        "      # Explicitly expand pos_encoding to match spatial dimensions of x before addition\n",
        "      # pos_encoding shape is (batch_size, channels)\n",
        "      # x shape is (batch_size, channels, H, W)\n",
        "      # Unsqueeze twice to get (batch_size, channels, 1, 1), then expand\n",
        "      pos_encoding_expanded = pos_encoding.unsqueeze(-1).unsqueeze(-1).expand_as(x)\n",
        "      x = x + pos_encoding_expanded\n",
        "\n",
        "      x, _ = self.bottleneck(x, is_bottleneck=True)\n",
        "\n",
        "\n",
        "      for dec in self.decoder:\n",
        "        x = dec(x, skip_connections.pop())\n",
        "\n",
        "      return x # This is predicted epsilon\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.encode = self.conv_block(in_channels=in_channels, out_channel=out_channels)\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2) # Changed kernel and stride\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3, # Changed kernel_size from 4 to 3\n",
        "                stride=1,      # Changed stride from 2 to 1\n",
        "                padding=1      # Added padding\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3, # Changed kernel_size from 4 to 3\n",
        "                stride=1,      # Changed stride from 2 to 1\n",
        "                padding=1      # Added padding\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, is_bottleneck=False):\n",
        "        x = self.encode(x)\n",
        "        if is_bottleneck:\n",
        "            return x, None # Bottleneck doesn't produce skip for itself\n",
        "        else:\n",
        "            skip = x\n",
        "            x = self.max_pooling(x)\n",
        "            return x, skip\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.decode = self.conv_block(in_channels=out_channels + in_channels, out_channel=out_channels) # Corrected in_channels for decode\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3, # Changed kernel_size from 4 to 3\n",
        "                stride=1,      # Changed stride from 2 to 1\n",
        "                padding=1      # Added padding\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3, # Changed kernel_size from 4 to 3\n",
        "                stride=1,      # Changed stride from 2 to 1\n",
        "                padding=1      # Added padding\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.up(x)\n",
        "        # Handle potential size mismatch due to ConvTranspose2d\n",
        "        target_h, target_w = skip_connection.size(2), skip_connection.size(3)\n",
        "        if x.size(2) != target_h or x.size(3) != target_w:\n",
        "            diff_h = target_h - x.size(2)\n",
        "            diff_w = target_w - x.size(3)\n",
        "\n",
        "            # Apply padding or cropping\n",
        "            if diff_h > 0 or diff_w > 0:\n",
        "                x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
        "                              diff_h // 2, diff_h - diff_h // 2])\n",
        "            elif diff_h < 0 or diff_w < 0:\n",
        "                x = x[:, :, :target_h, :target_w]\n",
        "\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.decode(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1OqWMrwn_aCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "# forward process is used to predict the noisy latent for the given timestamp\n",
        "def forward_process(z_0, time_step, epsilon=None, max_timestamp=1000):\n",
        "    # Define beta schedule (linear from 0.0001 to 0.02)\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(beta_start, beta_end, max_timestamp, device=z_0.device)\n",
        "\n",
        "    # Calculate alpha and cumulative product of alphas (alpha_bar)\n",
        "    alphas = 1.0 - betas\n",
        "    # alphas_cumprod[t] is the product of (1 - beta_s) from s=1 to t\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    # Get alpha_bar_t for each time_step in the batch\n",
        "    # time_step is a tensor of shape (batch_size,) with values from 1 to MAX_TIMESTAMP\n",
        "    # We use time_step - 1 for 0-indexed access to alphas_cumprod\n",
        "    alpha_bar_t_batch = alphas_cumprod[time_step - 1]\n",
        "\n",
        "    # Reshape alpha_bar_t_batch to be broadcastable with z_0 and epsilon\n",
        "    # e.g., (batch_size,) -> (batch_size, 1, 1, 1) for image tensors\n",
        "    alpha_bar_t_batch = alpha_bar_t_batch.view(-1, 1, 1, 1)\n",
        "\n",
        "    # Apply the diffusion formula: z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
        "    z_t = torch.sqrt(alpha_bar_t_batch) * z_0 + torch.sqrt(1 - alpha_bar_t_batch) * epsilon\n",
        "    return z_t\n",
        "\n",
        "\n",
        "# Training process for unet\n",
        "def train_model():\n",
        "    print(\"Training started...\")\n",
        "    dataloader = data_loader(dataset_path, max_length=100) # Using max_length=10 for quicker testing\n",
        "    # when we are trying to train unet , unet input will be 28x28 or latent size?\n",
        "    # it is latent\n",
        "\n",
        "    vae = VAE(in_channels=3, image_size=128, blocks=2) # Changed blocks from 1 to 2\n",
        "    vae.load_state_dict(torch.load(models_dump + \"/vae_model.pth\"))\n",
        "    out_channels = vae.get_out_channels()\n",
        "    unet = UNet(in_channels=out_channels)\n",
        "    MAX_TIMESTAMP = 200\n",
        "    epochs = 5 # Reduced epochs for quicker testing\n",
        "\n",
        "    optimizer = optim.Adam(unet.parameters(), lr=0.001)\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            t = torch.randint(1, MAX_TIMESTAMP, (data.size(0),), device=data.device).long()\n",
        "\n",
        "            # Ensure VAE is in eval mode during UNet training to prevent VAE weights from changing\n",
        "            vae.eval()\n",
        "            with torch.no_grad(): # No need to calculate gradients for VAE during UNet training\n",
        "              latent, z, mean, log_variance = vae.encode(data)  # Get the VAE latent\n",
        "\n",
        "            # add some noise here\n",
        "            epsilon = torch.randn_like(z)\n",
        "            # Pass MAX_TIMESTAMP to forward_process\n",
        "            noisy_z_t = forward_process(z_0=z, time_step=t, epsilon=epsilon, max_timestamp=MAX_TIMESTAMP)\n",
        "\n",
        "            pred_epsiloin = unet(noisy_z_t, t)\n",
        "            loss = torch.mean((pred_epsiloin - epsilon) ** 2)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "    print(\"Diffusion Training completed.\")\n",
        "    dump_path = models_dump + \"/diffusion_model.pth\"\n",
        "    # Note: Saving VAE state dict here, likely intended to save UNet's state dict\n",
        "    torch.save(unet.state_dict(), dump_path)\n",
        "    print(f\"Diffusion model saved to {dump_path}\")\n",
        "\n",
        "    # Save losses for later use\n",
        "    torch.save(\n",
        "        {\n",
        "          'train_losses': train_losses\n",
        "        }, 'diffusion_loss_history.pt'\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_loss_curves():\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Subplot 1: Total Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_losses, label='Total Loss')\n",
        "    plt.title('Total Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "# train_model()"
      ],
      "metadata": {
        "id": "CyxZfMZc_a-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "zIgnkJK5QeKb",
        "outputId": "1d32da92-2397-44c0-ff80-051c4b8b191a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Inference generation\n",
        "\n",
        "def normalize_image_tensor(image_tensor):\n",
        "    # Ensure the tensor is on CPU and convert to numpy\n",
        "    image_np = image_tensor.detach().cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
        "    # Normalize to [0, 1]\n",
        "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min() + 1e-8) # Add epsilon to avoid division by zero\n",
        "    return image_np\n",
        "\n",
        "def generate_image(unet, vae, config=None):\n",
        "    print(\"Inference started...\")\n",
        "\n",
        "    MAX_TIMESTAMP = 1000 # Changed to 1000 to match training\n",
        "\n",
        "    # vae = VAE(image_size=128, blocks=2)\n",
        "    vae.load_state_dict(torch.load(models_dump + \"/vae_model.pth\"))\n",
        "    vae.eval() # Set VAE to evaluation mode\n",
        "    out_channels = vae.get_out_channels()\n",
        "\n",
        "    # unet = UNet(in_channels=out_channels)\n",
        "    unet.load_state_dict(torch.load(models_dump + \"/diffusion_model.pth\"))\n",
        "    unet.eval() # Set UNet to evaluation mode\n",
        "\n",
        "    # Initialize z_t as pure noise in the latent space\n",
        "    # The latent z has shape (batch_size, out_channels, 32, 32) for image_size=128, blocks=2\n",
        "    # Use the device of the vae parameters\n",
        "    device = vae.parameters().__next__().device\n",
        "    z_t = torch.randn((1, out_channels, 32, 32), device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for t in tqdm(range(MAX_TIMESTAMP - 1, 0, -1)):\n",
        "          # Pass 'z_t' (the noisy latent) to UNet, consistent with training\n",
        "          predict_epsilon = unet(z_t, torch.tensor([t], device=z_t.device))\n",
        "          z_t = reverse_process_noisy_latent(z_t, t, predict_epsilon, MAX_TIMESTAMP=MAX_TIMESTAMP)\n",
        "\n",
        "    print(\"Inference completed.\")\n",
        "    # Decode the final latent representation\n",
        "    return vae.decode(z_t)\n",
        "\n",
        "def reverse_process_noisy_latent(z_t, t, ep, MAX_TIMESTAMP):\n",
        "    # Define beta schedule (linear from 0.0001 to 0.02) - Changed beta_start to match forward_process\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    EPS = 1e-8\n",
        "    # Ensure b_t is created for all required timesteps up to MAX_TIMESTAMP\n",
        "    b_t = torch.linspace(beta_start, beta_end, MAX_TIMESTAMP, device=z_t.device)\n",
        "    alpha_t = 1. - b_t\n",
        "    product_beta_t = torch.cumprod(alpha_t, dim=0)\n",
        "\n",
        "    # Select the correct beta and alpha_cumprod for the current timestep t\n",
        "    beta_t_scalar = b_t[t-1] # Adjust for 0-indexing\n",
        "    alpha_t_scalar = alpha_t[t-1]\n",
        "    product_beta_t_scalar = product_beta_t[t-1]\n",
        "\n",
        "    # Apply the reverse diffusion formula\n",
        "    z_t_minus_1 = 1 / torch.sqrt(alpha_t_scalar).clamp(EPS) * (z_t - (beta_t_scalar / torch.sqrt(1 - product_beta_t_scalar).clamp(EPS)) * ep)\n",
        "    # Add noise only if t > 1\n",
        "    if t > 1:\n",
        "        z_t_minus_1 = z_t_minus_1 + torch.sqrt(beta_t_scalar).clamp(EPS) * torch.randn_like(z_t)\n",
        "\n",
        "    return z_t_minus_1\n",
        "\n",
        "vae = VAE(image_size=128, blocks=2)\n",
        "out_channels = vae.get_out_channels()\n",
        "unet = UNet(in_channels=out_channels)\n",
        "# decoded = generate_image(vae=vae, unet=unet)\n",
        "# image_np = normalize_image_tensor(decoded)\n",
        "# plt.imshow(image_np)\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8oBjceYTQ0V",
        "outputId": "b19443e2-1d5c-46a2-ef07-2d808be484c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In vae channels out : 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert model for mobile - LiteRT\n",
        "\n",
        "# Uninstall potentially conflicting packages first\n",
        "!pip uninstall -y tensorflow ai-edge-tensorflow ai-edge-torch\n",
        "\n",
        "!pip install ai-edge-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JP8Xkzky2r-w",
        "outputId": "6b289e0e-a8b5-46ef-c43a-355928e68c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Found existing installation: ai_edge_tensorflow 2.21.0.dev20251110\n",
            "Uninstalling ai_edge_tensorflow-2.21.0.dev20251110:\n",
            "  Successfully uninstalled ai_edge_tensorflow-2.21.0.dev20251110\n",
            "Found existing installation: ai-edge-torch 0.7.1\n",
            "Uninstalling ai-edge-torch-0.7.1:\n",
            "  Successfully uninstalled ai-edge-torch-0.7.1\n",
            "Collecting ai-edge-torch\n",
            "  Using cached ai_edge_torch-0.7.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (1.16.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (0.7.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (1.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (4.57.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (0.3.13)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (0.9.0)\n",
            "Requirement already satisfied: torch<2.10.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (2.9.0+cu126)\n",
            "Collecting ai-edge-tensorflow==2.21.0.dev20251110 (from ai-edge-torch)\n",
            "  Using cached ai_edge_tensorflow-2.21.0.dev20251110-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: ai-edge-litert<2.2.0,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (2.1.0)\n",
            "Requirement already satisfied: ai-edge-quantizer==0.4.* in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (0.4.1)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from ai-edge-torch) (0.7.2)\n",
            "Requirement already satisfied: torch-xla2>=0.0.1.dev20241201 in /usr/local/lib/python3.12/dist-packages (from torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch) (0.0.1.dev202412041639)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from ai-edge-quantizer==0.4.*->ai-edge-torch) (4.2.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from ai-edge-quantizer==0.4.*->ai-edge-torch) (0.5.4)\n",
            "Requirement already satisfied: backports.strenum in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert<2.2.0,>=2.0.3->ai-edge-torch) (1.2.8)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert<2.2.0,>=2.0.3->ai-edge-torch) (25.12.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert<2.2.0,>=2.0.3->ai-edge-torch) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert<2.2.0,>=2.0.3->ai-edge-torch) (4.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert<2.2.0,>=2.0.3->ai-edge-torch) (6.33.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (1.76.0)\n",
            "Requirement already satisfied: tb-nightly~=2.20.0.a in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (2.20.0a20250717)\n",
            "Requirement already satisfied: keras-nightly>=3.10.0.dev in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.14.0.dev2026011404)\n",
            "Requirement already satisfied: h5py<3.15.0,>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (3.20.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.10.0,>=2.4.0->ai-edge-torch) (3.5.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch) (8.4.2)\n",
            "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax->ai-edge-torch) (0.7.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->ai-edge-torch) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->ai-edge-torch) (0.36.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->ai-edge-torch) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->ai-edge-torch) (0.22.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.45.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->ai-edge-torch) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-nightly>=3.10.0.dev->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras-nightly>=3.10.0.dev->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras-nightly>=3.10.0.dev->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.10.0,>=2.4.0->ai-edge-torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly~=2.20.0.a->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.10.0,>=2.4.0->ai-edge-torch) (3.0.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-nightly>=3.10.0.dev->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.10.0.dev->ai-edge-tensorflow==2.21.0.dev20251110->ai-edge-torch) (0.1.2)\n",
            "Using cached ai_edge_torch-0.7.1-py3-none-any.whl (457 kB)\n",
            "Using cached ai_edge_tensorflow-2.21.0.dev20251110-cp312-cp312-manylinux_2_27_x86_64.whl (268.5 MB)\n",
            "Installing collected packages: ai-edge-tensorflow, ai-edge-torch\n",
            "Successfully installed ai-edge-tensorflow-2.21.0.dev20251110 ai-edge-torch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ai_edge_torch\n",
        "import torch.fx # Added import for symbolic tracing\n",
        "import ai_edge_torch.quantize.pt2e_quantizer as pt2e_quantizer # Corrected import\n",
        "from ai_edge_torch.quantize import quant_config\n",
        "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e # Explicitly import prepare_pt2e and convert_pt2e\n",
        "import torch.export # Import torch.export\n",
        "\n",
        "\n",
        "class DiffusionUnetWrapper(nn.Module):\n",
        "  def __init__(self, unet) -> None:\n",
        "    super().__init__()\n",
        "    self.unet = unet\n",
        "\n",
        "  def forward(self, x, time_step): # Corrected signature to accept time_step\n",
        "    if time_step.dim() == 1:\n",
        "          time_step = time_step.view(-1)\n",
        "\n",
        "    return self.unet(x, time_step)\n",
        "\n",
        "\n",
        "class VAEEncoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    # During quantization and compilation, torch.randn_like might cause issues.\n",
        "    # For TFLite conversion, we return the deterministic mean of the latent space.\n",
        "    latent = self.vae.encoder(x)\n",
        "    mean = self.vae.mean(latent)\n",
        "    # The log_variance and reparameterization trick are omitted here for deterministic conversion.\n",
        "    return mean # Return mean instead of z for TFLite conversion\n",
        "\n",
        "\n",
        "class VAEDecoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.vae.decode(x)\n",
        "\n",
        "\n",
        "def convert_to_tflite(\n",
        "    model: nn.Module,\n",
        "    sample_input, # Can be a single Tensor or a tuple of Tensors\n",
        "    output_path: str,\n",
        "    quantize: bool = True\n",
        "):\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # edge_model = ai_edge_torch.convert(\n",
        "    #   model,\n",
        "    #   sample_input,\n",
        "    # )\n",
        "    # path = output_path + \"fp32.tflite\"\n",
        "\n",
        "    # edge_model.save(\n",
        "    #     os.path.join(models_dump, path)\n",
        "    # )\n",
        "\n",
        "    if isinstance(sample_input, torch.Tensor):\n",
        "        sample_input = (sample_input,)\n",
        "    elif isinstance(sample_input, list):\n",
        "        sample_input = tuple(sample_input)\n",
        "    elif not isinstance(sample_input, tuple):\n",
        "        raise ValueError(f\"sample_input must be Tensor, list, or tuple. Got {type(sample_input)}\")\n",
        "\n",
        "    if quantize:\n",
        "        quantizer = pt2e_quantizer.PT2EQuantizer().set_global(\n",
        "            pt2e_quantizer.get_symmetric_quantization_config()\n",
        "        )\n",
        "\n",
        "        edge_model = ai_edge_torch.convert(\n",
        "            model,\n",
        "            sample_input,\n",
        "            quant_config=quant_config.QuantConfig(pt2e_quantizer=quantizer),\n",
        "        )\n",
        "    else:\n",
        "        edge_model = ai_edge_torch.convert(model, sample_input)\n",
        "\n",
        "    # Save to file\n",
        "    edge_model.export(output_path)\n",
        "    print(f\"✓ Model saved to {output_path}\")\n",
        "\n",
        "    # Print file size\n",
        "    import os\n",
        "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "    print(f\"✓ Model size: {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "MXUkk5M3GleT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "collapsed": true,
        "outputId": "cafe1321-1dad-4e18-b626-07da4eeec590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "/usr/local/lib/python3.12/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15TensorShapeBaseINS_11TensorShapeEEC2EN4absl12lts_202308024SpanIKlEE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1550025589.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;31m# Added import for symbolic tracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt2e_quantizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt2e_quantizer\u001b[0m \u001b[0;31m# Corrected import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_pt2e\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_pt2e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_pt2e\u001b[0m \u001b[0;31m# Explicitly import prepare_pt2e and convert_pt2e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental_add_compilation_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/_convert/converter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msignature_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mai_edge_litert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfl_interpreter\u001b[0m  \u001b[0;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0m_kernel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kernels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_kernel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0m_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_kernel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# Load third party dynamic kernels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/load_library.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_libraries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mpy_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_LoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /usr/local/lib/python3.12/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15TensorShapeBaseINS_11TensorShapeEEC2EN4absl12lts_202308024SpanIKlEE"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ai_edge_torch\n",
        "import torch.fx # Added import for symbolic tracing\n",
        "import ai_edge_torch.quantize.pt2e_quantizer as pt2e_quantizer # Corrected import\n",
        "from ai_edge_torch.quantize import quant_config\n",
        "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e # Explicitly import prepare_pt2e and convert_pt2e\n",
        "import torch.export # Import torch.export\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# --- VAE Components (fully defined here for self-containment) ---\n",
        "class VAEEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim, block_count=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        curr_channels = in_channels\n",
        "        for i in range(block_count):\n",
        "            out_channel = 64 * (2 ** i)\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=out_channel,\n",
        "                )\n",
        "            )\n",
        "            curr_channels = out_channel\n",
        "\n",
        "        self.out_channels = out_channel\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, image_tensor):\n",
        "        latent = image_tensor\n",
        "        for block in self.blocks:\n",
        "            latent = block(latent)\n",
        "        return latent\n",
        "\n",
        "\n",
        "class VAEDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=128, out_channels=3, blocks=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = 3 if i == (blocks - 1) else in_channels // 2\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=next_channels\n",
        "                )\n",
        "            )\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        x = latent\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VAE class definition is needed for vae_model_load\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3, image_size=32, blocks=2):\n",
        "        super().__init__()\n",
        "        self.encoder = VAEEncoder(\n",
        "            in_channels=in_channels,\n",
        "            latent_dim=(image_size / (2 ** blocks)),\n",
        "            block_count=blocks\n",
        "        )\n",
        "        self.out_channels_val = self.encoder.out_channels # Store it correctly\n",
        "\n",
        "        latent_channels = self.encoder.out_channels\n",
        "\n",
        "        self.mean = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "        self.log_variance = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "\n",
        "        self.decoder = VAEDecoder(\n",
        "            in_channels=latent_channels, out_channels=3, blocks=blocks\n",
        "        )\n",
        "\n",
        "    def get_out_channels(self):\n",
        "        return self.out_channels_val\n",
        "\n",
        "    def reparameterized_trick(self, mean, log_variance):\n",
        "        std = torch.exp(0.5 * log_variance)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        z = mean + std * epsilon\n",
        "        return z\n",
        "\n",
        "    def encode(self, image_tensor):\n",
        "        latent = self.encoder(image_tensor)\n",
        "\n",
        "        mean = self.mean(latent)\n",
        "        log_variance = self.log_variance(latent)\n",
        "\n",
        "        z = self.reparameterized_trick(mean=mean, log_variance=log_variance)\n",
        "\n",
        "        return latent, z, mean, log_variance\n",
        "\n",
        "    def decode(self, latent):\n",
        "        x = self.decoder(latent)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- UNet Components (fully defined here for self-containment) ---\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.encode = self.conv_block(in_channels=in_channels, out_channel=out_channels)\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, is_bottleneck=False):\n",
        "        x = self.encode(x)\n",
        "        if is_bottleneck:\n",
        "            return x, None\n",
        "        else:\n",
        "            skip = x\n",
        "            x = self.max_pooling(x)\n",
        "            return x, skip\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.decode = self.conv_block(in_channels=out_channels + in_channels, out_channel=out_channels)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.up(x)\n",
        "        target_h, target_w = skip_connection.size(2), skip_connection.size(3)\n",
        "        if x.size(2) != target_h or x.size(3) != target_w:\n",
        "            diff_h = target_h - x.size(2)\n",
        "            diff_w = target_w - x.size(3)\n",
        "            if diff_h > 0 or diff_w > 0:\n",
        "                x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
        "                              diff_h // 2, diff_h - diff_h // 2])\n",
        "            elif diff_h < 0 or diff_w < 0:\n",
        "                x = x[:, :, :target_h, :target_w]\n",
        "\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, blocks=2):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.blocks = blocks\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels * 2\n",
        "            self.encoder.append(EncoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "        self.bottleneck = EncoderBlock(curr_channels, curr_channels)\n",
        "\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels // 2\n",
        "            self.decoder.append(DecoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def sinusoidal_embedding(self, time_step, embedding_dim):\n",
        "      device = time_step.device\n",
        "      dtype = time_step.dtype\n",
        "      half_dim = embedding_dim // 2\n",
        "      # FIX: Ensure tensors are created on the correct device and with the correct dtype\n",
        "      emb = torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / (half_dim - 1)\n",
        "      emb = torch.exp(torch.arange(half_dim, device=device, dtype=dtype) * -emb)\n",
        "      emb = time_step[:, None] * emb[None, :]\n",
        "      emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
        "      return emb\n",
        "\n",
        "\n",
        "    def forward(self, z_t, time_step):\n",
        "      x = z_t\n",
        "      skip_connections = []\n",
        "      for enc in self.encoder:\n",
        "        x, s_c = enc(x)\n",
        "        skip_connections.append(s_c)\n",
        "\n",
        "      pos_encoding = self.sinusoidal_embedding(time_step, self.final_encoder_channels)\n",
        "\n",
        "      # Create pos_encoding_expanded in NHWC format (as observed in the error message)\n",
        "      # pos_encoding: (B, C)\n",
        "      # unsqueeze(1) -> (B, 1, C)\n",
        "      # unsqueeze(1) -> (B, 1, 1, C)\n",
        "      # repeat(1, H, W, 1) -> (B, H, W, C)\n",
        "      pos_encoding_expanded = pos_encoding.unsqueeze(1).unsqueeze(1).repeat(1, 8, 8, 1)\n",
        "\n",
        "      # Permute pos_encoding_expanded from NHWC (B, H, W, C) to NCHW (B, C, H, W)\n",
        "      # to match the NCHW format of 'x' before addition.\n",
        "      pos_encoding_expanded = pos_encoding_expanded.permute(0, 3, 1, 2)\n",
        "\n",
        "      # Now both 'x' and 'pos_encoding_expanded' are in NCHW format for addition\n",
        "      x = x + pos_encoding_expanded\n",
        "\n",
        "      x, _ = self.bottleneck(x, is_bottleneck=True)\n",
        "\n",
        "\n",
        "      for dec in self.decoder:\n",
        "        x = dec(x, skip_connections.pop())\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "# --- Wrapper Classes ---\n",
        "class DiffusionUnetWrapper(nn.Module):\n",
        "  def __init__(self, unet) -> None:\n",
        "    super().__init__()\n",
        "    self.unet = unet\n",
        "\n",
        "  def forward(self, x, time_step):\n",
        "    if time_step.dim() == 1:\n",
        "          time_step = time_step.view(-1)\n",
        "\n",
        "    return self.unet(x, time_step)\n",
        "\n",
        "\n",
        "class VAEEncoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    latent = self.vae.encoder(x)\n",
        "    mean = self.vae.mean(latent)\n",
        "    return mean\n",
        "\n",
        "\n",
        "class VAEDecoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.vae.decode(x)\n",
        "\n",
        "\n",
        "# --- TFLite Conversion Function ---\n",
        "def convert_to_tflite(\n",
        "    model: nn.Module,\n",
        "    sample_input,\n",
        "    output_path: str,\n",
        "    quantize: bool = True\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sample_input, torch.Tensor):\n",
        "        sample_input = (sample_input,)\n",
        "    elif isinstance(sample_input, list):\n",
        "        sample_input = tuple(sample_input)\n",
        "    elif not isinstance(sample_input, tuple):\n",
        "        raise ValueError(f\"sample_input must be Tensor, list, or tuple. Got {type(sample_input)}\")\n",
        "\n",
        "    if quantize:\n",
        "        quantizer = pt2e_quantizer.PT2EQuantizer().set_global(\n",
        "            pt2e_quantizer.get_symmetric_quantization_config()\n",
        "        )\n",
        "\n",
        "        edge_model = ai_edge_torch.convert(\n",
        "            model,\n",
        "            sample_input,\n",
        "            quant_config=quant_config.QuantConfig(pt2e_quantizer=quantizer),\n",
        "        )\n",
        "    else:\n",
        "        edge_model = ai_edge_torch.convert(model, sample_input)\n",
        "\n",
        "    edge_model.export(output_path)\n",
        "    print(f\"✓ Model saved to {output_path}\")\n",
        "\n",
        "    import os\n",
        "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "    print(f\"✓ Model size: {size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "# --- Model Loading and Conversion Execution ---\n",
        "# Re-initialize VAE and UNet to ensure they are on CPU if not already, for conversion.\n",
        "vae_model_load = VAE(image_size=128, blocks=2)\n",
        "vae_model_load.load_state_dict(torch.load(models_dump + \"/vae_model.pth\"))\n",
        "vae_model_load.eval()\n",
        "vae_model_load.to('cpu') # Move to CPU\n",
        "vae_out_channels = vae_model_load.get_out_channels()\n",
        "\n",
        "\n",
        "unet_model_load = UNet(in_channels=vae_out_channels)\n",
        "unet_model_load.load_state_dict(torch.load(models_dump + \"/diffusion_model.pth\"))\n",
        "unet_model_load.eval()\n",
        "unet_model_load.to('cpu') # Move to CPU\n",
        "\n",
        "# Test forward pass with the loaded UNet model\n",
        "test_latent = torch.randn(1, vae_out_channels, 32, 32)\n",
        "test_timestep = torch.tensor([500])\n",
        "try:\n",
        "    output = unet_model_load(test_latent, test_timestep)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"✓ Forward pass successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Forward pass failed: {e}\")\n",
        "\n",
        "mobile_unet = DiffusionUnetWrapper(unet_model_load)\n",
        "mobile_vae_decoder = VAEDecoderWrapper(vae_model_load)\n",
        "mobile_vae_encoder = VAEEncoderWrapper(vae_model_load)\n",
        "\n",
        "# --- VAE Encoder Conversion ---\n",
        "image_input_for_encoder = torch.randn(1, 3, 128, 128) # Input for VAEEncoderWrapper\n",
        "convert_to_tflite(model=mobile_vae_encoder,\n",
        "        sample_input=(image_input_for_encoder,),\n",
        "        output_path=os.path.join(models_dump, \"vae_encoder_quantized.tflite\"), # Unique filename\n",
        "        quantize=True)\n",
        "\n",
        "# --- UNet Conversion ---\n",
        "# Example input for UNet: latent tensor and time step\n",
        "# Corrected latent space shape to 32x32 based on VAE output\n",
        "example_latent_input = torch.randn(1, vae_out_channels, 32, 32) # Corrected spatial dimensions\n",
        "example_timestep_input = torch.tensor([500]) # Example timestep for UNet\n",
        "\n",
        "convert_to_tflite(model=mobile_unet,\n",
        "        sample_input=(example_latent_input, example_timestep_input), # UNet takes two inputs\n",
        "        output_path=os.path.join(models_dump, \"diffusion_unet_quantized.tflite\"), # Unique filename\n",
        "        quantize=False)\n",
        "\n",
        "# --- VAE Decoder Conversion ---\n",
        "# The input to the VAE decoder is a latent tensor, similar to the UNet's latent input.\n",
        "input_for_decoder = torch.randn(1, vae_out_channels, 32, 32) # Example latent input for decoder\n",
        "convert_to_tflite(model=mobile_vae_decoder,\n",
        "        sample_input=(input_for_decoder,),\n",
        "        output_path=os.path.join(models_dump, \"vae_decoder_quantized.tflite\"), # Unique filename\n",
        "        quantize=True)"
      ],
      "metadata": {
        "id": "_cIUZ6cr3JCL",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "outputId": "2f59d1ca-bba8-4f25-bfe3-991bdd8b0fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:351: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✗ Forward pass failed: 'UNet' object has no attribute 'final_encoder_channels'\n",
            "✓ Model saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/vae_encoder_quantized.tflite\n",
            "✓ Model size: 1.52 MB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Unsupported",
          "evalue": "Observed exception\n  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.\n  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.\n  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.\n\n  Developer debug context: raised exception AttributeError([])\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0088.html\n\nfrom user code:\n   File \"/tmp/ipython-input-924052956.py\", line 287, in forward\n    return self.unet(x, time_step)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-924052956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0mexample_timestep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Example timestep for UNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m convert_to_tflite(model=mobile_unet,\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0msample_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_latent_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_timestep_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# UNet takes two inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"diffusion_unet_quantized.tflite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Unique filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-924052956.py\u001b[0m in \u001b[0;36mconvert_to_tflite\u001b[0;34m(model, sample_input, output_path, quantize)\u001b[0m\n\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0medge_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mai_edge_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0medge_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/_convert/converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(module, sample_args, sample_kwargs, strict_export, quant_config, dynamic_shapes, _ai_edge_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0m_ai_edge_converter_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m   return Converter().convert(\n\u001b[0m\u001b[1;32m    316\u001b[0m       \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0msample_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/_convert/converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, module, sample_args, sample_kwargs, strict_export, quant_config, dynamic_shapes, _ai_edge_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;34m\" specified.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         )\n\u001b[0;32m--> 203\u001b[0;31m     converted_model = conversion.convert_signatures(\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mstrict_export\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict_export\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/_convert/conversion.py\u001b[0m in \u001b[0;36mconvert_signatures\u001b[0;34m(signatures, strict_export, quant_config, _tfl_converter_flags, _saved_model_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   exported_programs: torch.export.ExportedProgram = [\n\u001b[0;32m--> 140\u001b[0;31m       export(\n\u001b[0m\u001b[1;32m    141\u001b[0m           \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ai_edge_torch/_convert/conversion.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mexported_program\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0mexported_program\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     exported_program = fx_infra.graph_utils.reset_from_node_meta(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mnew_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdraft_export_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         return _export(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 )\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0m_EXPORT_FLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m             log_export_usage(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/exported_program.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0munset_fake_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m   2253\u001b[0m     \u001b[0;31m# while internally it returns False UNLESS otherwise specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexport_training_ir_rollout_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m         ep = _export_for_training(\n\u001b[0m\u001b[1;32m   2256\u001b[0m             \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 )\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0m_EXPORT_FLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m             log_export_usage(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/exported_program.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0munset_fake_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_export_for_training\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, prefer_deferred_runtime_asserts_over_guards)\u001b[0m\n\u001b[1;32m   2069\u001b[0m         ]\n\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2071\u001b[0;31m     export_artifact = export_func(\n\u001b[0m\u001b[1;32m   2072\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_strict_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, orig_in_spec, prefer_deferred_runtime_asserts_over_guards, _to_aten_func)\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \"\"\"\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m     gm_torch_level = _export_to_torch_ir(\n\u001b[0m\u001b[1;32m   1416\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_export_to_torch_ir\u001b[0;34m(f, args, kwargs, dynamic_shapes, preserve_module_call_signature, disable_constraint_solver, prefer_deferred_runtime_asserts_over_guards, restore_fqn, _log_export_usage, same_signature)\u001b[0m\n\u001b[1;32m    810\u001b[0m                 )\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ignore_backend_decomps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m                 gm_torch_level, _ = torch._dynamo.export(\n\u001b[0m\u001b[1;32m    813\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                     \u001b[0mdynamic_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_shapes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;31m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m                 \u001b[0mresult_traced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConstraintViolationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m                 \u001b[0mconstraint_violation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     def __reduce__(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcompile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m                         \u001b[0mcur_exn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                         \u001b[0mcur_exn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_exn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m  \u001b[0;31m# User compiler error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mShortenTraceback\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Failures in the backend likely don't have useful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupported\u001b[0m: Observed exception\n  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.\n  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.\n  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.\n\n  Developer debug context: raised exception AttributeError([])\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0088.html\n\nfrom user code:\n   File \"/tmp/ipython-input-924052956.py\", line 287, in forward\n    return self.unet(x, time_step)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9005a5d1"
      },
      "source": [
        "# Task\n",
        "To address the `RuntimeError` during TFLite conversion of the UNet model due to dynamic shape inference in `sinusoidal_embedding` and `pos_encoding_expanded`, I will modify the `UNet` class:\n",
        "1.  **Explicit `final_encoder_channels`**: Define `self.final_encoder_channels` in the `UNet`'s `__init__` method and use this fixed value for the `embedding_dim` argument when calling `self.sinusoidal_embedding`.\n",
        "2.  **Hardcode Spatial Dimensions**: In the `UNet`'s `forward` method, hardcode the spatial dimensions (8, 8) in the `repeat` call for `pos_encoding_expanded` to ensure static shapes for TFLite.\n",
        "3.  **Disable Quantization for UNet**: Set `quantize=False` during the UNet conversion to isolate the shape inference issue.\n",
        "\n",
        "This approach ensures that the shapes are statically defined, which is crucial for TFLite conversion.\n",
        "\n",
        "```python\n",
        "import ai_edge_torch\n",
        "import torch.fx # Added import for symbolic tracing\n",
        "import ai_edge_torch.quantize.pt2e_quantizer as pt2e_quantizer # Corrected import\n",
        "from ai_edge_torch.quantize import quant_config\n",
        "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e # Explicitly import prepare_pt2e and convert_pt2e\n",
        "import torch.export # Import torch.export\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# --- VAE Components (fully defined here for self-containment) ---\n",
        "class VAEEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim, block_count=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        curr_channels = in_channels\n",
        "        for i in range(block_count):\n",
        "            out_channel = 64 * (2 ** i)\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=out_channel,\n",
        "                )\n",
        "            )\n",
        "            curr_channels = out_channel\n",
        "\n",
        "        self.out_channels = out_channel\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, image_tensor):\n",
        "        latent = image_tensor\n",
        "        for block in self.blocks:\n",
        "            latent = block(latent)\n",
        "        return latent\n",
        "\n",
        "\n",
        "class VAEDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=128, out_channels=3, blocks=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = 3 if i == (blocks - 1) else in_channels // 2\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=next_channels\n",
        "                )\n",
        "            )\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        x = latent\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VAE class definition is needed for vae_model_load\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3, image_size=32, blocks=2):\n",
        "        super().__init__()\n",
        "        self.encoder = VAEEncoder(\n",
        "            in_channels=in_channels,\n",
        "            latent_dim=(image_size / (2 ** blocks)),\n",
        "            block_count=blocks\n",
        "        )\n",
        "        self.out_channels_val = self.encoder.out_channels # Store it correctly\n",
        "\n",
        "        latent_channels = self.encoder.out_channels\n",
        "\n",
        "        self.mean = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "        self.log_variance = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "\n",
        "        self.decoder = VAEDecoder(\n",
        "            in_channels=latent_channels, out_channels=3, blocks=blocks\n",
        "        )\n",
        "\n",
        "    def get_out_channels(self):\n",
        "        return self.out_channels_val\n",
        "\n",
        "    def reparameterized_trick(self, mean, log_variance):\n",
        "        std = torch.exp(0.5 * log_variance)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        z = mean + std * epsilon\n",
        "        return z\n",
        "\n",
        "    def encode(self, image_tensor):\n",
        "        latent = self.encoder(image_tensor)\n",
        "\n",
        "        mean = self.mean(latent)\n",
        "        log_variance = self.log_variance(latent)\n",
        "\n",
        "        z = self.reparameterized_trick(mean=mean, log_variance=log_variance)\n",
        "\n",
        "        return latent, z, mean, log_variance\n",
        "\n",
        "    def decode(self, latent):\n",
        "        x = self.decoder(latent)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- UNet Components (fully defined here for self-containment) ---\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.encode = self.conv_block(in_channels=in_channels, out_channel=out_channels)\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, is_bottleneck=False):\n",
        "        x = self.encode(x)\n",
        "        if is_bottleneck:\n",
        "            return x, None\n",
        "        else:\n",
        "            skip = x\n",
        "            x = self.max_pooling(x)\n",
        "            return x, skip\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.decode = self.conv_block(in_channels=out_channels + in_channels, out_channel=out_channels)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.up(x)\n",
        "        target_h, target_w = skip_connection.size(2), skip_connection.size(3)\n",
        "        if x.size(2) != target_h or x.size(3) != target_w:\n",
        "            diff_h = target_h - x.size(2)\n",
        "            diff_w = target_w - x.size(3)\n",
        "            if diff_h > 0 or diff_w > 0:\n",
        "                x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
        "                              diff_h // 2, diff_h - diff_h // 2])\n",
        "            elif diff_h < 0 or diff_w < 0:\n",
        "                x = x[:, :, :target_h, :target_w]\n",
        "\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, blocks=2):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.blocks = blocks\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels * 2\n",
        "            self.encoder.append(EncoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "        # FIX 1: Explicitly define final_encoder_channels\n",
        "        self.final_encoder_channels = curr_channels\n",
        "\n",
        "        self.bottleneck = EncoderBlock(curr_channels, curr_channels)\n",
        "\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels // 2\n",
        "            self.decoder.append(DecoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def sinusoidal_embedding(self, time_step, embedding_dim):\n",
        "      device = time_step.device\n",
        "      dtype = time_step.dtype\n",
        "      half_dim = embedding_dim // 2\n",
        "      # FIX: Ensure tensors are created on the correct device and with the correct dtype\n",
        "      emb = torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / (half_dim - 1)\n",
        "      emb = torch.exp(torch.arange(half_dim, device=device, dtype=dtype) * -emb)\n",
        "      emb = time_step[:, None] * emb[None, :]\n",
        "      emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
        "      return emb\n",
        "\n",
        "\n",
        "    def forward(self, z_t, time_step):\n",
        "      x = z_t\n",
        "      skip_connections = []\n",
        "      for enc in self.encoder:\n",
        "        x, s_c = enc(x)\n",
        "        skip_connections.append(s_c)\n",
        "\n",
        "      # FIX 1: Use the explicitly stored final_encoder_channels\n",
        "      pos_encoding = self.sinusoidal_embedding(time_step, self.final_encoder_channels)\n",
        "      # FIX 2: Hardcode spatial dimensions (8, 8)\n",
        "      pos_encoding_expanded = pos_encoding.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 8, 8)\n",
        "      x = x + pos_encoding_expanded\n",
        "\n",
        "      x, _ = self.bottleneck(x, is_bottleneck=True)\n",
        "\n",
        "\n",
        "      for dec in self.decoder:\n",
        "        x = dec(x, skip_connections.pop())\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "# --- Wrapper Classes ---\n",
        "class DiffusionUnetWrapper(nn.Module):\n",
        "  def __init__(self, unet) -> None:\n",
        "    super().__init__()\n",
        "    self.unet = unet\n",
        "\n",
        "  def forward(self, x, time_step):\n",
        "    if time_step.dim() == 1:\n",
        "          time_step = time_step.view(-1)\n",
        "\n",
        "    return self.unet(x, time_step)\n",
        "\n",
        "\n",
        "class VAEEncoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    latent = self.vae.encoder(x)\n",
        "    mean = self.vae.mean(latent)\n",
        "    return mean\n",
        "\n",
        "\n",
        "class VAEDecoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.vae.decode(x)\n",
        "\n",
        "\n",
        "# --- TFLite Conversion Function ---\n",
        "def convert_to_tflite(\n",
        "    model: nn.Module,\n",
        "    sample_input,\n",
        "    output_path: str,\n",
        "    quantize: bool = True\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sample_input, torch.Tensor):\n",
        "        sample_input = (sample_input,)\n",
        "    elif isinstance(sample_input, list):\n",
        "        sample_input = tuple(sample_input)\n",
        "    elif not isinstance(sample_input, tuple):\n",
        "        raise ValueError(f\"sample_input must be Tensor, list, or tuple. Got {type(sample_input)}\")\n",
        "\n",
        "    if quantize:\n",
        "        quantizer = pt2e_quantizer.PT2EQuantizer().set_global(\n",
        "            pt2e_quantizer.get_symmetric_quantization_config()\n",
        "        )\n",
        "\n",
        "        edge_model = ai_edge_torch.convert(\n",
        "            model,\n",
        "            sample_input,\n",
        "            quant_config=quant_config.QuantConfig(pt2e_quantizer=quantizer),\n",
        "        )\n",
        "    else:\n",
        "        edge_model = ai_edge_torch.convert(model, sample_input)\n",
        "\n",
        "    edge_model.export(output_path)\n",
        "    print(f\"✓ Model saved to {output_path}\")\n",
        "\n",
        "    import os\n",
        "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "    print(f\"✓ Model size: {size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "# --- Model Loading and Conversion Execution ---\n",
        "# Re-initialize VAE and UNet to ensure they are on CPU if not already, for conversion.\n",
        "vae_model_load = VAE(image_size=128, blocks=2)\n",
        "vae_model_load.load_state_dict(torch.load(models_dump + \"/vae_model.pth\"))\n",
        "vae_model_load.eval()\n",
        "vae_model_load.to('cpu') # Move to CPU\n",
        "vae_out_channels = vae_model_load.get_out_channels()\n",
        "\n",
        "\n",
        "unet_model_load = UNet(in_channels=vae_out_channels)\n",
        "unet_model_load.load_state_dict(torch.load(models_dump + \"/diffusion_model.pth\"))\n",
        "unet_model_load.eval()\n",
        "unet_model_load.to('cpu') # Move to CPU\n",
        "\n",
        "# Test forward pass with the loaded UNet model\n",
        "test_latent = torch.randn(1, vae_out_channels, 32, 32)\n",
        "test_timestep = torch.tensor([500])\n",
        "try:\n",
        "    output = unet_model_load(test_latent, test_timestep)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"✓ Forward pass successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Forward pass failed: {e}\")\n",
        "\n",
        "mobile_unet = DiffusionUnetWrapper(unet_model_load)\n",
        "mobile_vae_decoder = VAEDecoderWrapper(vae_model_load)\n",
        "mobile_vae_encoder = VAEEncoderWrapper(vae_model_load)\n",
        "\n",
        "# --- VAE Encoder Conversion ---\n",
        "image_input_for_encoder = torch.randn(1, 3, 128, 128) # Input for VAEEncoderWrapper\n",
        "convert_to_tflite(model=mobile_vae_encoder,\n",
        "        sample_input=(image_input_for_encoder,),\n",
        "        output_path=os.path.join(models_dump, \"vae_encoder_quantized.tflite\"), # Unique filename\n",
        "        quantize=True)\n",
        "\n",
        "# --- UNet Conversion ---\n",
        "# Example input for UNet: latent tensor and time step\n",
        "# Corrected latent space shape to 32x32 based on VAE output\n",
        "example_latent_input = torch.randn(1, vae_out_channels, 32, 32) # Corrected spatial dimensions\n",
        "example_timestep_input = torch.tensor([500]) # Example timestep for UNet\n",
        "\n",
        "convert_to_tflite(model=mobile_unet,\n",
        "        sample_input=(example_latent_input, example_timestep_input), # UNet takes two inputs\n",
        "        output_path=os.path.join(models_dump, \"diffusion_unet_quantized.tflite\"), # Unique filename\n",
        "        quantize=False) # FIX 3: Set quantize=False for UNet\n",
        "\n",
        "# --- VAE Decoder Conversion ---\n",
        "# The input to the VAE decoder is a latent tensor, similar to the UNet's latent input.\n",
        "input_for_decoder = torch.randn(1, vae_out_channels, 32, 32) # Example latent input for decoder\n",
        "convert_to_tflite(model=mobile_vae_decoder,\n",
        "        sample_input=(input_for_decoder,),\n",
        "        output_path=os.path.join(models_dump, \"vae_decoder_quantized.tflite\"), # Unique filename\n",
        "        quantize=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17b63cfe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db720876-a5f4-4852-fba5-68920adbb2ee"
      },
      "source": [
        "import ai_edge_torch\n",
        "import torch.fx # Added import for symbolic tracing\n",
        "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e # Explicitly import prepare_pt2e and convert_pt2e\n",
        "import torch.export # Import torch.export\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from ai_edge_torch.quantize import quant_config, pt2e_quantizer\n",
        "\n",
        "# --- VAE Components (fully defined here for self-containment) ---\n",
        "class VAEEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim, block_count=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        curr_channels = in_channels\n",
        "        for i in range(block_count):\n",
        "            out_channel = 64 * (2 ** i)\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=out_channel,\n",
        "                )\n",
        "            )\n",
        "            curr_channels = out_channel\n",
        "\n",
        "        self.out_channels = out_channel\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, image_tensor):\n",
        "        latent = image_tensor\n",
        "        for block in self.blocks:\n",
        "            latent = block(latent)\n",
        "        return latent\n",
        "\n",
        "\n",
        "class VAEDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=128, out_channels=3, blocks=2):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = 3 if i == (blocks - 1) else in_channels // 2\n",
        "            self.blocks.append(\n",
        "                self.conv_block(\n",
        "                    in_channels=curr_channels,\n",
        "                    out_channel=next_channels\n",
        "                )\n",
        "            )\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        x = latent\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VAE class definition is needed for vae_model_load\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3, image_size=32, blocks=2):\n",
        "        super().__init__()\n",
        "        self.encoder = VAEEncoder(\n",
        "            in_channels=in_channels,\n",
        "            latent_dim=(image_size / (2 ** blocks)),\n",
        "            block_count=blocks\n",
        "        )\n",
        "        self.out_channels_val = self.encoder.out_channels # Store it correctly\n",
        "\n",
        "        latent_channels = self.encoder.out_channels\n",
        "\n",
        "        self.mean = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "        self.log_variance = nn.Conv2d(in_channels=latent_channels, out_channels=latent_channels, kernel_size=4, padding=1)\n",
        "\n",
        "        self.decoder = VAEDecoder(\n",
        "            in_channels=latent_channels,\n",
        "            out_channels=3,\n",
        "            blocks=blocks\n",
        "        )\n",
        "\n",
        "    def get_out_channels(self):\n",
        "        return self.out_channels_val\n",
        "\n",
        "    def reparameterized_trick(self, mean, log_variance):\n",
        "        std = torch.exp(0.5 * log_variance)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        z = mean + std * epsilon\n",
        "        return z\n",
        "\n",
        "    def encode(self, image_tensor):\n",
        "        latent = self.encoder(image_tensor)\n",
        "\n",
        "        mean = self.mean(latent)\n",
        "        log_variance = self.log_variance(latent)\n",
        "\n",
        "        z = self.reparameterized_trick(mean=mean, log_variance=log_variance)\n",
        "\n",
        "        return latent, z, mean, log_variance\n",
        "\n",
        "    def decode(self, latent):\n",
        "        x = self.decoder(latent)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- UNet Components (fully defined here for self-containment) ---\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.encode = self.conv_block(in_channels=in_channels, out_channel=out_channels)\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, is_bottleneck=False):\n",
        "        x = self.encode(x)\n",
        "        if is_bottleneck:\n",
        "            return x, None\n",
        "        else:\n",
        "            skip = x\n",
        "            x = self.max_pooling(x)\n",
        "            return x, skip\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.decode = self.conv_block(in_channels=out_channels + in_channels, out_channel=out_channels)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channel):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channel,\n",
        "                out_channels=out_channel,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.up(x)\n",
        "        target_h, target_w = skip_connection.size(2), skip_connection.size(3)\n",
        "        if x.size(2) != target_h or x.size(3) != target_w:\n",
        "            diff_h = target_h - x.size(2)\n",
        "            diff_w = target_w - x.size(3)\n",
        "            if diff_h > 0 or diff_w > 0:\n",
        "                x = F.pad(x, [diff_w // 2, diff_w - diff_w // 2,\n",
        "                              diff_h // 2, diff_h - diff_h // 2])\n",
        "            elif diff_h < 0 or diff_w < 0:\n",
        "                x = x[:, :, :target_h, :target_w]\n",
        "\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, blocks=2):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.blocks = blocks\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "\n",
        "        curr_channels = in_channels\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels * 2\n",
        "            self.encoder.append(EncoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "        self.final_encoder_channels = curr_channels\n",
        "\n",
        "        self.bottleneck = EncoderBlock(curr_channels, curr_channels)\n",
        "\n",
        "        for i in range(blocks):\n",
        "            next_channels = curr_channels // 2\n",
        "            self.decoder.append(DecoderBlock(curr_channels, next_channels))\n",
        "\n",
        "            curr_channels = next_channels\n",
        "\n",
        "    def sinusoidal_embedding(self, time_step, embedding_dim):\n",
        "      device = time_step.device\n",
        "      dtype = time_step.dtype\n",
        "      half_dim = embedding_dim // 2\n",
        "      emb = torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / (half_dim - 1)\n",
        "      emb = torch.exp(torch.arange(half_dim, device=device, dtype=dtype) * -emb)\n",
        "      emb = time_step[:, None] * emb[None, :]\n",
        "      emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
        "      return emb\n",
        "\n",
        "\n",
        "    def forward(self, z_t, time_step):\n",
        "      x = z_t\n",
        "      skip_connections = []\n",
        "      for enc in self.encoder:\n",
        "        x, s_c = enc(x)\n",
        "        skip_connections.append(s_c)\n",
        "\n",
        "      pos_encoding = self.sinusoidal_embedding(time_step, self.final_encoder_channels)\n",
        "\n",
        "      # Create pos_encoding_expanded in NHWC format (as observed in the error message)\n",
        "      # pos_encoding: (B, C)\n",
        "      # unsqueeze(1) -> (B, 1, C)\n",
        "      # unsqueeze(1) -> (B, 1, 1, C)\n",
        "      # repeat(1, H, W, 1) -> (B, H, W, C)\n",
        "      pos_encoding_expanded = pos_encoding.unsqueeze(1).unsqueeze(1).repeat(1, 8, 8, 1)\n",
        "\n",
        "      # Permute pos_encoding_expanded from NHWC (B, H, W, C) to NCHW (B, C, H, W)\n",
        "      # to match the NCHW format of 'x' before addition.\n",
        "      pos_encoding_expanded = pos_encoding_expanded.permute(0, 3, 1, 2)\n",
        "\n",
        "      # Now both 'x' and 'pos_encoding_expanded' are in NCHW format for addition\n",
        "      x = x + pos_encoding_expanded\n",
        "\n",
        "      x, _ = self.bottleneck(x, is_bottleneck=True)\n",
        "\n",
        "\n",
        "      for dec in self.decoder:\n",
        "        x = dec(x, skip_connections.pop())\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "# --- Wrapper Classes ---\n",
        "class DiffusionUnetWrapper(nn.Module):\n",
        "  def __init__(self, unet) -> None:\n",
        "    super().__init__()\n",
        "    self.unet = unet\n",
        "\n",
        "  def forward(self, x, time_step):\n",
        "    if time_step.dim() == 1:\n",
        "          time_step = time_step.view(-1)\n",
        "\n",
        "    return self.unet(x, time_step)\n",
        "\n",
        "\n",
        "class VAEEncoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    latent = self.vae.encoder(x)\n",
        "    mean = self.vae.mean(latent)\n",
        "    return mean\n",
        "\n",
        "\n",
        "class VAEDecoderWrapper(nn.Module):\n",
        "  def __init__(self, vae) -> None:\n",
        "    super().__init__()\n",
        "    self.vae = vae\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.vae.decode(x)\n",
        "\n",
        "\n",
        "# --- TFLite Conversion Function ---\n",
        "def convert_to_tflite(\n",
        "    model: nn.Module,\n",
        "    sample_input,\n",
        "    output_path: str,\n",
        "    quantize: bool = True\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sample_input, torch.Tensor):\n",
        "        sample_input = (sample_input,)\n",
        "    elif isinstance(sample_input, list):\n",
        "        sample_input = tuple(sample_input)\n",
        "    elif not isinstance(sample_input, tuple):\n",
        "        raise ValueError(f\"sample_input must be Tensor, list, or tuple. Got {type(sample_input)}\")\n",
        "\n",
        "    if quantize:\n",
        "        quantizer = pt2e_quantizer.PT2EQuantizer().set_global(\n",
        "            pt2e_quantizer.get_symmetric_quantization_config()\n",
        "        )\n",
        "\n",
        "        edge_model = ai_edge_torch.convert(\n",
        "            model,\n",
        "            sample_input,\n",
        "            quant_config=quant_config.QuantConfig(pt2e_quantizer=quantizer),\n",
        "        )\n",
        "    else:\n",
        "        edge_model = ai_edge_torch.convert(model, sample_input)\n",
        "\n",
        "    edge_model.export(output_path)\n",
        "    print(f\"✓ Model saved to {output_path}\")\n",
        "\n",
        "    import os\n",
        "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "    print(f\"✓ Model size: {size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "# --- Model Loading and Conversion Execution ---\n",
        "# Re-initialize VAE and UNet to ensure they are on CPU if not already, for conversion.\n",
        "vae_model_load = VAE(image_size=128, blocks=2)\n",
        "vae_model_load.load_state_dict(torch.load(models_dump + \"/vae_model.pth\"))\n",
        "vae_model_load.eval()\n",
        "vae_model_load.to('cpu') # Move to CPU\n",
        "vae_out_channels = vae_model_load.get_out_channels()\n",
        "\n",
        "\n",
        "unet_model_load = UNet(in_channels=vae_out_channels)\n",
        "unet_model_load.load_state_dict(torch.load(models_dump + \"/diffusion_model.pth\"))\n",
        "unet_model_load.eval()\n",
        "unet_model_load.to('cpu') # Move to CPU\n",
        "\n",
        "# Test forward pass with the loaded UNet model\n",
        "test_latent = torch.randn(1, vae_out_channels, 32, 32)\n",
        "test_timestep = torch.tensor([500])\n",
        "try:\n",
        "    output = unet_model_load(test_latent, test_timestep)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"✓ Forward pass successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Forward pass failed: {e}\")\n",
        "\n",
        "mobile_unet = DiffusionUnetWrapper(unet_model_load)\n",
        "mobile_vae_decoder = VAEDecoderWrapper(vae_model_load)\n",
        "mobile_vae_encoder = VAEEncoderWrapper(vae_model_load)\n",
        "\n",
        "# --- VAE Encoder Conversion ---\n",
        "image_input_for_encoder = torch.randn(1, 3, 128, 128) # Input for VAEEncoderWrapper\n",
        "convert_to_tflite(model=mobile_vae_encoder,\n",
        "        sample_input=(image_input_for_encoder,),\n",
        "        output_path=os.path.join(models_dump, \"vae_encoder_quantized.tflite\"), # Unique filename\n",
        "        quantize=True)\n",
        "\n",
        "# --- UNet Conversion ---\n",
        "# Example input for UNet: latent tensor and time step\n",
        "# Corrected latent space shape to 32x32 based on VAE output\n",
        "example_latent_input = torch.randn(1, vae_out_channels, 32, 32) # Corrected spatial dimensions\n",
        "example_timestep_input = torch.tensor([500]) # Example timestep for UNet\n",
        "\n",
        "convert_to_tflite(model=mobile_unet,\n",
        "        sample_input=(example_latent_input, example_timestep_input), # UNet takes two inputs\n",
        "        output_path=os.path.join(models_dump, \"diffusion_unet_quantized.tflite\"), # Unique filename\n",
        "        quantize=True) # Set quantize to False as requested\n",
        "\n",
        "# --- VAE Decoder Conversion ---\n",
        "# The input to the VAE decoder is a latent tensor, similar to the UNet's latent input.\n",
        "input_for_decoder = torch.randn(1, vae_out_channels, 32, 32) # Example latent input for decoder\n",
        "convert_to_tflite(model=mobile_vae_decoder,\n",
        "        sample_input=(input_for_decoder,),\n",
        "        output_path=os.path.join(models_dump, \"vae_decoder_quantized.tflite\"), # Unique filename\n",
        "        quantize=True)\n",
        "\n",
        "for n, p in mobile_unet.named_parameters():\n",
        "    print(n, p.dtype)\n",
        "\n",
        "# this means"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 128, 32, 32])\n",
            "✓ Forward pass successful!\n",
            "✓ Model saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/vae_encoder_quantized.tflite\n",
            "✓ Model size: 1.52 MB\n",
            "✓ Model saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/diffusion_unet_quantized.tflite\n",
            "✓ Model size: 48.66 MB\n",
            "✓ Model saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/vae_decoder_quantized.tflite\n",
            "✓ Model size: 0.52 MB\n",
            "unet.encoder.0.encode.0.weight torch.float32\n",
            "unet.encoder.0.encode.0.bias torch.float32\n",
            "unet.encoder.0.encode.2.weight torch.float32\n",
            "unet.encoder.0.encode.2.bias torch.float32\n",
            "unet.encoder.1.encode.0.weight torch.float32\n",
            "unet.encoder.1.encode.0.bias torch.float32\n",
            "unet.encoder.1.encode.2.weight torch.float32\n",
            "unet.encoder.1.encode.2.bias torch.float32\n",
            "unet.decoder.0.up.weight torch.float32\n",
            "unet.decoder.0.up.bias torch.float32\n",
            "unet.decoder.0.decode.0.weight torch.float32\n",
            "unet.decoder.0.decode.0.bias torch.float32\n",
            "unet.decoder.0.decode.2.weight torch.float32\n",
            "unet.decoder.0.decode.2.bias torch.float32\n",
            "unet.decoder.1.up.weight torch.float32\n",
            "unet.decoder.1.up.bias torch.float32\n",
            "unet.decoder.1.decode.0.weight torch.float32\n",
            "unet.decoder.1.decode.0.bias torch.float32\n",
            "unet.decoder.1.decode.2.weight torch.float32\n",
            "unet.decoder.1.decode.2.bias torch.float32\n",
            "unet.bottleneck.encode.0.weight torch.float32\n",
            "unet.bottleneck.encode.0.bias torch.float32\n",
            "unet.bottleneck.encode.2.weight torch.float32\n",
            "unet.bottleneck.encode.2.bias torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U onnx onnxscript"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_ujAtIuOlog_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets try out the ONNX -> tflite\n",
        "\n",
        "torch.onnx.export(\n",
        "    mobile_unet,\n",
        "    (example_latent_input, example_timestep_input),\n",
        "    \"diffusion_unet.onnx\",\n",
        "    opset_version=17,\n",
        "    input_names=[\"latent\", \"timestep\"],\n",
        "    output_names=[\"noise_pred\"],\n",
        "    do_constant_folding=True,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XIcfy821hn9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-tf tensorflow"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x3VNE7OTiRp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "onnx_model = onnx.load(\"diffusion_unet.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"ONNX model is valid\")"
      ],
      "metadata": {
        "id": "Srdo7uWXiNnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y onnx-tf"
      ],
      "metadata": {
        "id": "4QbJXYQYojA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onnx-tf convert -i diffusion_unet.onnx -o diffusion_unet_tf"
      ],
      "metadata": {
        "id": "3gDqwaCkiZEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def representative_dataset():\n",
        "    for _ in range(100):\n",
        "        latent = np.random.randn(1, vae_out_channels, 32, 32).astype(np.float32)\n",
        "        timestep = np.array([500], dtype=np.int64)\n",
        "        yield [latent, timestep]\n"
      ],
      "metadata": {
        "id": "NRWC-vLyiqlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"diffusion_unet_tf\")\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "# FULL integer quantization\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
        "]\n",
        "\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"diffusion_unet_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"INT8 TFLite model saved\")"
      ],
      "metadata": {
        "id": "utw11yFai5GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9653f5"
      },
      "source": [
        "!onnx-tf convert -i diffusion_unet.onnx -o diffusion_unet_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh diffusion_unet_int8.tflite"
      ],
      "metadata": {
        "id": "D5KI-x5Si63X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"diffusion_unet_int8.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "for d in interpreter.get_tensor_details():\n",
        "    print(d[\"name\"], d[\"dtype\"])"
      ],
      "metadata": {
        "id": "Ei-qnR1zjA8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def quantize_and_convert(model, sample_input, output_path, model_name):\n",
        "    \"\"\"Quantize in PyTorch then convert to TFLite\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.to('cpu')\n",
        "\n",
        "    # Apply PyTorch dynamic quantization\n",
        "    print(f\"\\n🔧 Quantizing {model_name} in PyTorch...\")\n",
        "    quantized_model = torch.quantization.quantize_dynamic(\n",
        "        model,\n",
        "        {torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d},\n",
        "        dtype=torch.qint8\n",
        "    )\n",
        "\n",
        "    # Test quantized model\n",
        "    print(f\"Testing quantized {model_name}...\")\n",
        "    if isinstance(sample_input, tuple):\n",
        "        test_input = sample_input\n",
        "    else:\n",
        "        test_input = (sample_input,)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            output = quantized_model(*test_input)\n",
        "            print(f\"✓ Quantized model works! Output shape: {output.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Quantized model failed: {e}\")\n",
        "            print(\"Falling back to original model...\")\n",
        "            quantized_model = model\n",
        "\n",
        "    # Convert to TFLite\n",
        "    print(f\"Converting {model_name} to TFLite...\")\n",
        "    edge_model = ai_edge_torch.convert(quantized_model, test_input)\n",
        "\n",
        "    # Save with proper naming\n",
        "    if \"quantized\" not in output_path:\n",
        "        output_path = output_path.replace('.tflite', '_pytorch_quant.tflite')\n",
        "\n",
        "    edge_model.export(output_path)\n",
        "\n",
        "    # Report size\n",
        "    import os\n",
        "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "    print(f\"✓ {model_name} saved to {output_path}\")\n",
        "    print(f\"✓ Size: {size_mb:.2f} MB\\n\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# Apply to all models\n",
        "print(\"=\"*60)\n",
        "print(\"PYTORCH QUANTIZATION + TFLITE CONVERSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# VAE Encoder\n",
        "vae_enc_path = quantize_and_convert(\n",
        "    model=mobile_vae_encoder,\n",
        "    sample_input=(torch.randn(1, 3, 128, 128),),\n",
        "    output_path=os.path.join(models_dump, \"vae_encoder_quant.tflite\"),\n",
        "    model_name=\"VAE Encoder\"\n",
        ")\n",
        "\n",
        "# UNet\n",
        "unet_path = quantize_and_convert(\n",
        "    model=mobile_unet,\n",
        "    sample_input=(example_latent_input, example_timestep_input),\n",
        "    output_path=os.path.join(models_dump, \"diffusion_unet_quant.tflite\"),\n",
        "    model_name=\"UNet\"\n",
        ")\n",
        "\n",
        "# VAE Decoder\n",
        "vae_dec_path = quantize_and_convert(\n",
        "    model=mobile_vae_decoder,\n",
        "    sample_input=(torch.randn(1, vae_out_channels, 32, 32),),\n",
        "    output_path=os.path.join(models_dump, \"vae_decoder_quant.tflite\"),\n",
        "    model_name=\"VAE Decoder\"\n",
        ")\n",
        "\n",
        "# Verify quantization worked\n",
        "print(\"=\"*60)\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "verify_quantization(vae_enc_path)\n",
        "verify_quantization(unet_path)\n",
        "verify_quantization(vae_dec_path)"
      ],
      "metadata": {
        "id": "axe_EsohoYTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "914984c1-dddf-4d89-81d4-fce024080147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PYTORCH QUANTIZATION + TFLITE CONVERSION\n",
            "============================================================\n",
            "\n",
            "🔧 Quantizing VAE Encoder in PyTorch...\n",
            "Testing quantized VAE Encoder...\n",
            "✓ Quantized model works! Output shape: torch.Size([1, 128, 31, 31])\n",
            "Converting VAE Encoder to TFLite...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3220197142.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ VAE Encoder saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/vae_encoder_quant_pytorch_quant.tflite\n",
            "✓ Size: 1.52 MB\n",
            "\n",
            "\n",
            "🔧 Quantizing UNet in PyTorch...\n",
            "Testing quantized UNet...\n",
            "✓ Quantized model works! Output shape: torch.Size([1, 128, 32, 32])\n",
            "Converting UNet to TFLite...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3220197142.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ UNet saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/diffusion_unet_quant_pytorch_quant.tflite\n",
            "✓ Size: 48.66 MB\n",
            "\n",
            "\n",
            "🔧 Quantizing VAE Decoder in PyTorch...\n",
            "Testing quantized VAE Decoder...\n",
            "✓ Quantized model works! Output shape: torch.Size([1, 3, 128, 128])\n",
            "Converting VAE Decoder to TFLite...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3220197142.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ VAE Decoder saved to /content/drive/MyDrive/Colab Notebooks/models/stable_diffusion_training/vae_decoder_quant_pytorch_quant.tflite\n",
            "✓ Size: 0.52 MB\n",
            "\n",
            "============================================================\n",
            "VERIFICATION\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'verify_quantization' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3220197142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VERIFICATION\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mverify_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_enc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mverify_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mverify_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_dec_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'verify_quantization' is not defined"
          ]
        }
      ]
    }
  ]
}